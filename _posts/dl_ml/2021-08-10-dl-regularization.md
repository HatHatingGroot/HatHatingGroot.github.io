---
title: "[DL/ML]Regularization"
categories:
  - DL-ML
tags:
  - DeepLearning
  - Regularization
  - EalryStopping
  - ParameterNormPenalty
  - DataArgumentation
  - NoiseRobustness
  - LabelSmoothing
  - Dropout
  - BatchNormalization
last_modified_at: 2021-08-10T00:00:00-00:00
---


## Summary ğŸ¤™
<hr/>
Generalizationì„ ë†’ì´ê¸° ìœ„í•´ í•™ìŠµì„ ë°©í•´í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë‹¤ì‹œ ë§í•´ test setì—ì„œ ì˜ ë™ì‘í•˜ê²Œ í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤.

<br><br/>


## Index ğŸ‘€       
  * [Early Stopping](#early-stopping)
  * [Parameter Norm Penalty](#parameter-norm-penalty)
  * [Data argumentation](#data-argumentation)
  * [Noise robustness](#noise-robustness)
  * [Label smoothing](#label-smoothing)
  * [Dropout](#Dropout)
  * [Batch normalization](#batch-normalization)

<br/>


## Early Stopping
<hr/>

ì¶”ê°€ì ì¸ Validation Dataë¥¼ êµ¬ì„±í•˜ê³ , Training Errorì™€ Validation Errorì˜ ì°¨ì´ê°€ ì¦ê°€í•  ë•Œ í•™ìŠµì„ ë©ˆì¶”ëŠ” ë°©ë²•ì´ë‹¤.

<br><br/>

## Parameter Norm Penalty
<hr/>

$$
\begin{aligned}
&\text { total } \operatorname{cost}=\operatorname{loss}(\mathcal{D} ; W)+\frac{\alpha}{2}\|W\|_{2}^{2}
\end{aligned}
$$

Weight Parameterì˜ í¬ê¸°(ì ˆëŒ€ê°’)ë¥¼ ì •ê·œí™”í•˜ì—¬ ì‘ê²Œ ë§Œë“œëŠ” ê²ƒì´ë‹¤.    
ë¶€ë“œëŸ¬ìš´ í•¨ìˆ˜ê°€ Generalization ì„±ëŠ¥ì´ ë†’ì„ ê²ƒì´ë¼ëŠ” ê°€ì •ì—ì„œ ì¶œë°œí•œë‹¤.

<br><br/>


## Data argumentation
<hr/>

ë°ì´í„°ê°€ ë§ì„ ìˆ˜ë¡ ì„±ëŠ¥ì´ ë†’ì•„ì§„ë‹¤. ë‹¨, ë°ì´í„°ê°€ í•œì •ì ì¸ ê²½ìš°ì— Labelì´ ë³€í•˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì—¬ ë°ì´í„° ì…‹ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

<br><br/>


## Noise Robustness
<hr/>

ì…ë ¥ì´ë‚˜ Weightì— Noiseë¥¼ ì¤‘ê°„ì— ì§€ì†ì ìœ¼ë¡œ ë„£ì–´ì£¼ë©´ ë” í•™ìŠµë¥ ì´ ë†’ì•„ì§ˆ ìˆ˜ ìˆë‹¤. (ì´ìœ ëŠ” ì•„ì§ ëª¨ë¦„)

<br><br/>


## Label smoothing
<hr/>

* Mixup : ë‘ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì„ê³  labelë„ ì„ëŠ”ë‹¤.
* Cutout : ì´ë¯¸ì§€ì˜ ì¼ì • ì˜ì—­ì„ ëº€ë‹¤.
* CutMix : íŠ¹ì • ì˜ì—­ì„ ì˜ë¼ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ë„£ëŠ”ë‹¤.

<br><br/>


## Dropout
<hr/>

weightì˜ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” í•˜ì—¬ íŠ¹ì • featureì— êµ­í•œë˜ì§€ ì•Šë„ë¡ í•œë‹¤. 


<br><br/>



## Batch Normalization
<hr/>

$$
\begin{aligned}
\mu_{B} &=\frac{1}{m} \sum_{i=1}^{m} x_{i} \\
\sigma_{B}^{2} &=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2} \\
\hat{x}_{i} &=\frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}}
\end{aligned}
$$

 layerì˜ weight íŒŒë¼ë¯¸í„°ë¥¼ ì •ê·œí™”í•œë‹¤. (ë…¼ë€ì´ ë§ìœ¼ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.)

<br><br/>