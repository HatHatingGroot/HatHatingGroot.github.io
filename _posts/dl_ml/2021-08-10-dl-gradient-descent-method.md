---
title: "[DL/ML]Gradient Descent Methods"
categories:
  - 
tags:
  - 
last_modified_at: 2021-08-10T00:00:00-00:00
---


## Summary ğŸ¤™
<hr/>
Gradient DescentëŠ” ë‹¤ìŒìœ¼ë¡œ ê°„ë‹¨íˆ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.    

* Stochastic Gradient Descent : í•œê°œì˜ ìƒ˜í”Œë¡œë¶€í„° gradientë¥¼ ê³„ì‚°í•˜ì—¬ ì—…ë°ì´íŠ¸   
* Mini-batch Gradient Descent : ë°ì´í„°ì˜ subsetìœ¼ë¡œ gradientë¥¼ ê³„ì‚°í•˜ì—¬ ì—…ë°ì´íŠ¸   
* Batch Gradient Descent : ëª¨ë“  ë°ì´í„°ë¡œ gradientë¥¼ ê³„ì‚°í•˜ì—¬ ì—…ë°ì´íŠ¸    

> ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ í¬ë©´ sharp minimizersì— ìˆ˜ë ´í•˜ëŠ” ê²½í–¥ì´ ìˆê³ , ì‘ìœ¼ë©´ flat minimizerì— ìˆ˜ë ´í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.
>  [On Large-batch Training for Deep Learning Generalization Gap and Sharp Minima, 2017](https://arxiv.org/abs/1609.04836)
<br><br/>


## Index ğŸ‘€       
  * [Stochastic Gradient Descent](#stochastic-gradient-descent)
  * [Momentum](#momentum)
  * [Nesterov accelerated gradient](#nesterov-accelerated-gradientnag)
  * [Adagrad](#adagrad)
  * [Adadelta](#adadelta)
  * [RMSprop](#rmsprop)
  * [Adam](#adam)

<br/>


## Stochastic Gradient Descent
<hr/>

$$ W_{t+1} =  W_t - \eta g_t $$    

ê°€ì¥ ê¸°ë³¸ì ì¸ Gradient Descentì˜ ë°©ë²•ì´ë‹¤.   

>ì´ë•Œ Learning Rateë¥¼ ë‚˜íƒ€ë‚´ëŠ” hyperparameter $\eta$ë¥¼ ì ì ˆíˆ ì„¤ì •í•´ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.    

<br><br/>

## Momentum
<hr/>

$$\begin{aligned} a_{t+1} & \leftarrow \beta a_{t}+g_{t} \\ W_{t+1} & \leftarrow W_{t}-\eta a_{t+1} \end{aligned}$$

ì´ì „ í•™ìŠµì˜ ê´€ì„±ì„ ìœ ì§€ì‹œì¼œ Gradientê°€ ë³€í•˜ë”ë¼ë„ ì´ì „ ì •ë³´ë¥¼ í™œìš©í•œë‹¤.   
$\beta$ëŠ” momentumì˜ ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” hyper parmeterì´ë‹¤.    
$a_t$ëŠ” accumulationìœ¼ë¡œì„œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ë° ì‹¤ì œë¡œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.   

> ë‹¤ë§Œ ê´€ì„±ìœ¼ë¡œ ì¸í•´ ë©ˆì¶°ì•¼í•  ì§€ì (local minima)ì— convergeí•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.

<br><br/>

## Nesterov accelerated gradient(NAG)
<hr/>

$$\begin{aligned} a_{t+1} & \leftarrow \beta a_{t}+\nabla \mathcal{L}\left(W_{t}-\eta \beta a_{t}\right) \\ W_{t+1} & \leftarrow W_{t}-\eta a_{t+1} \end{aligned}$$

Momentumì˜ ë‹¨ì ì„ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‹¤. Momentumì€ ê´€ì„±ê³¼ gradientë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°í•˜ì§€ë§Œ, NAGëŠ” momentumë§Œí¼ ë¨¼ì € ì´ë™í•œ ë’¤ gradientë¥¼ ê³„ì‚°í•œë‹¤. 

> ë”°ë¦¬ì„œ momentumì˜ ì¥ì ì€ ìœ ì§€í•˜ë©´ì„œ, ë©ˆì¶°ì•¼í•  ì§€ì ì— ë©ˆì¶”ëŠ”ë°ì— í›¨ì”¬ ìš©ì´í•˜ë‹¤.


<br><br/>


## Adagrad
<hr/>

$$W_{t+1}=W_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} g_{t}$$
$G_t$ : sum of gradient squares    
$\epsilon$ : for numerical stability    

Adaptive Gradientë¡œ íŒŒë¼ë¯¸í„° ê°’ë“¤ì˜ ë³€í™”ëŸ‰ì— ì˜í•´ Learning Rateë¥¼ ì¡°ì ˆí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë³€í™”ëŸ‰ì´ ë§ì€ íŒŒë¼ë¯¸í„°ëŠ” LearningRateë¥¼ ê°ì†Œì‹œí‚¤ê³ , ì ì€ íŒŒë¼ë¯¸í„°ëŠ” LearningRateë¥¼ ì¦ê°€ì‹œí‚¤ë„ë¡ ìœ ë„í•  ìˆ˜ ìˆë‹¤.    


>ë‹¨, í•™ìŠµì´ ì§„í–‰ë  ìˆ˜ë¡ $G_t$ ê°’ì´ ì¦ê°€í•˜ì—¬ LearningRateê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ë¬¸ì œ(__monotonically decreasing__)ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.     

<br><br/>



## Adadelta
<hr/>

$$
\begin{aligned}
G_{t} &=\gamma G_{t-1}+(1-\gamma) g_{t}^{2} \\
W_{t+1} &=W_{t}-\frac{\sqrt{H_{t-1}+\epsilon}}{\sqrt{G_{t}+\epsilon}} g_{t} \\
H_{t} &=\gamma H_{t-1}+(1-\gamma)\left(\Delta W_{t}\right)^{2}
\end{aligned}
$$

$G_t$ : EMA of gradient squares    
$H_t$ : EMA of difference squares    


Adagradì˜ ë¬¸ì œì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ [EMA(exponential moving average)](https://ko.wikipedia.org/wiki/%EC%9D%B4%EB%8F%99%ED%8F%89%EA%B7%A0#cite_note-3)ë¥¼ ì·¨í•œë‹¤. 

ê²°ê³¼ì ìœ¼ë¡œëŠ” ìµœê·¼ì— gradientì— ë”°ë¼ ë°˜ëŒ€ë¡œ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ëŠ” ê²ƒì´ë‹¤.

>íŠ¹ì§•ì€ ëª…ì‹œì ì¸ Learning Rateê°€ ì—†ë‹¤ëŠ” ì ì´ë‹¤.     
>ë”°ë¼ì„œ ì»¤ìŠ¤í…€ ì˜ì—­ì´ ì ì–´ ì‹¤ì œë¡œëŠ” ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.     


<br><br/>


## RMSprop
<hr/>

$$
\begin{aligned}
G_{t} &=\gamma G_{t-1}+(1-\gamma) g_{t}^{2} \\
W_{t+1} &=W_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} g_{t}
\end{aligned}
$$

$G_t$ : EMA of gradient squares  

ë…¼ë¬¸ì„ í†µí•´ ì†Œê°œëœ ë°©ë²•ë¡ ì€ ì•„ë‹ˆê³  Geoff Hintonì˜ ê°•ì˜ì—ì„œ ì†Œê°œëœ ë…íŠ¹í•œ ë°©ë²•ì´ë‹¤.    
>íŠ¹ì§•ì€ Adadeltaì— Learning rateê°€ ì¶”ê°€ë˜ì—ˆë‹¤ëŠ” ê²ƒì´ë‹¤.     

<br><br/>




## Adam
<hr/>

$$
\begin{aligned}
m_{t} &=\beta_{1} m_{t=1}+\left(1-\beta_{1}\right) g_{t} \\
v_{t} &=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \\
W_{t+1} &=W_{t}-\frac{\eta}{\sqrt{v_{t}+\epsilon}} \frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}} m_{t}
\end{aligned}
$$

$m_t$ : Momentum    
$v_t$ : EMA of gradient squares    

Adaptive Moment Estimation      

> ì•ì„œ ì†Œê°œëœ Momentumê³¼ Adaptive, ì´ ë‘ ê°œë…ì„ ì˜ ìš©í•©í•œ ë°©ë²•ì´ë‹¤. 


<br><br/>

