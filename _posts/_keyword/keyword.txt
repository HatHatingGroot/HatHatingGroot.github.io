-------------------------------------
** 포스팅 프레임
- 요약
- 배경, 이유
- 정의
- 성질
- 활용 사례
- 관련 키워드
- 코딩 구현(optional)

-------------------------------------

=========[[ linear algebra ]]=========
# norm
    - 정의
    - L1, L2 노름 활용 예시
        -- L1 : Robust 학습, Lasso회귀
        -- L2 : Laplace 근사, Ridge 회귀
    - 노름으로 각도구하기
# 역행렬
    - 정의
    - 조건(determenant)
# 의사 역행렬 (moore-penrose)
    - 정의 (의미)
    - 특징
    - 사용 예시

=========[[ Caculus ]]=========
# Gradient Descent (경사 하강법)
    - 정의
    - Gradient Vector
    - 활용 예시
    - 무어 펜로즈 역행렬을 활용한 방법
    - 미분을 활용한 방법
    - 특징 (한계, 전제)

# SGD; Stochastic Gradient Descent) 확률적 경사하강법

# 강의영상 03:47부터 소개되는 내용인, d-차원 벡터(베타)에 대한 그레디언트 벡터를 구하는 계산을 각자 직접 손으로 해보기 바랍니다!


=========[[ Probablity & Statistic ]]=========
# 주변확률분포, 결합분포, 확률분포

# 로지스틱 회귀

# 기댓값(공분산, 첨도)

# 몬테카를로 샘플링
    - 몬테카를로 방법을 활용하여 원주율에 대한 근삿값을 어떻게 구할 수 있을까요?

# 모집단, 모수, 확률분포, 

# 표집분포와 중심극한정리

# MLE 최대 가능도 추정법
    - 라그랑주 승수
    - 로그가능도
    
# 확률분포의 거리로서의 손실함수
    - 총변동거리 (Total Variation Distance, TV)
    - 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL)
    - 바슈타인 거리 (Wassertein Distance)

# 쿨백-라이블러 발산
    - 크로스엔트로피

# 베이즈 정리
    - 사후 확률, 사전확률, 가능도, 증거
    - 조건부 확률의 시각화
    - 사후확률 계산을 통한 정보의 갱신
    - 인과관계와 상관관계의 괴리 (심슨의 역설)

=========[[ General ]]=========
# 기계 학습과 딥러닝

# 손실함수

# 활성함수
    - softmax
    - ReLU
    - 시그모이드

# one-hot & softmax = 추론 & 분류
    - 분류 문제에서 softmax 함수가 사용되는 이유가 뭘까요?
    - softmax 함수의 결과값을 분류 모델의 학습에 어떤식으로 사용할 수 있을까요?

# CNN (Convolution Neural Network)
    - fully connected 에서 커널 기반으로



