---
title: "RNN(Recurrent Neural Network) Intro"
categories:
  - RNN
  - ML/DL
tags:
  - rnn
  - bptt
  - gradientdescent
last_modified_at: 2021-08-06T00:00:00-00:00
use_math: true
---

## memo(tmp)
1. i.i.d. : Independently and Identically distributed
2. CNN은 공간정보(spatial information)에 효율적인 반면, RNN은 시퀀스 정보에 더 적합하다.
   1. RNN은 과거 정보와 현재 입력 정보를 모두 가지는 state 변수를 통해 현재의 ouput을 정한다
3. RNN은 시계열 데이터, 주식시장 분석등에 활용된다.
4. 주가를 예로 들면 특정 시간 t에서의 x_t를 예측하고자 한다면
   1. $$ x_t ~ P(x_t|x_{t-1}, \cdots , x_1) $$
5. 여기서 문제는 t-1 ~ 1의 모든 시퀀스가 고려하는 것은 힘들기 때문에 다음의 전략으로 간소화 할수 있다.
   1. t-1 ~ 1의 모든 시퀀스 데이터가 필요한것이 아니라면 $\tau$ 만큼의 특정 구간을 정하여 $x_{t-1}, \cdots , x_{t-\tau}$만 관측한다.
   2. t에서의 hiddent state $h_t$를 통해 업데이트
   3. $P\left(x_{1}, \ldots, x_{T}\right)=\prod_{t=1}^{T} P\left(x_{t} \mid x_{t-1}, \ldots, x_{1}\right)$







## Summary 🤙
---
Gradient Descent (경사하강법)를 통해 함수 f의 극소값에 도달할 수 있다.


## Index 👀       
  * [Why](#why)
  * [Definition](#definition)
  * [Feature](#feature)
  * [Use Cases](#use-cases)
  * [Related](#related)
  * [Implementation](#implementation)

## Why 🤷
---
기계 학습에서 주로 사용되는 패턴은 특정 함수를 최소화하기 위해 반복적인 계산을 수행하는 것이다. Linear Regression을 위해 Loss함수를 최소화 하는 것이 대표적인 예이다. 
또한 Gradient Descent는 모든 차원과 공간에서 적용이 가능하다. 



## Definition 🧑‍🏫
---
변수 $x$에 대해 $f(x)$의 극솟값은 $f(x_k-f'(x_k))$가 0이 될 때까지 반복적으로 수행하면 도달할 수 있다.


## Feature 👍
---
TODO 


## Use Cases 🪢
---
TODO  


## Related 🧶
---
TODO 


## Implementation 😎
---
```python
# 1차원
import sympy as sym
from sympy.abc import x

f = sym.poly(x**2 + 2*x + 3)

val = init_value
grad = sym.diff(f)
```