---
title: "[DL/ML]Optimization - Important Concept"
categories:
  - DL-ML
tags:
  - optimization
last_modified_at: 2021-08-03T00:00:00-00:00
---


## Summary ğŸ¤™
<hr/>

"ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤"ë¼ëŠ” ê¸°ì¤€ë“¤ì„ ì•Œì•„ë³´ê³  ì–´ë–»ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³¸ë‹¤.     
>ë‹¨, ì–´ë–¤ ë°©ë²•ì´ë“  test dataë¥¼ í™œìš©í•˜ë©´ cheatingì„ì„ ëª…ì‹¬í•˜ì.

<br><br/>


## Index ğŸ‘€       
  * [Generalization](#generalization)
  * [fitting(Under|Over)](#underfitting--overfitting)
  * [Cross validation](#cross-validation)
  * [Bias-variance tradeoff](#bias-variance-tradeoff)
  * [BootStrapping](#bootstrapping)
  * [Bagging & Boosting](#bagging--boosting)

<br><br/>


## Generalization  
<hr/>
Trainning Errorì™€ Test Errorì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ ì´ Gapì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ ê°€ì§€ê²Œ ëœë‹¤.   
í›ˆë ¨ ë°ì´í„°ë¥¼ í†µí•´ ì–»ì€ Weightê°€ ì‹¤ì œ ë°ì´í„°(test data)ì—ì„œë„ ì–¼ë§ˆë‚˜ ì˜ ì ìš©ë˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.   

<br></br>

   

## Underfitting & Overfitting
<hr/>
__Underfitting__  ì´ë€ í•™ìŠµ ë°ì´í„°ì— ë§ëŠ” íŒŒë¼ë¯¸í„°ê°€ ëŠìŠ¨í•˜ê²Œ í•™ìŠµëœ í˜•íƒœë¡œì„œ ì ì ˆí•œ íŒŒë¼ë¯¸í„°ë¡œ íŒë‹¨í•˜ê¸° ì–´ë ¤ì›Œì§„ë‹¤.    
__Overfitting__ ì´ë€ í•™ìŠµ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ìµœì í™”ëœ ê²ƒìœ¼ë¡œì„œ ì•ì„œ ì–¸ê¸‰í•œ Generalization gapì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‹¤.    

  
<br></br>

## Cross-validation
<hr/>
K-fold Cross-validationì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.   
ë°ì´í„°ì…‹ì„ ì¼ì •í•œ í¬ê¸°ë¡œ k ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ 1ê°œì˜ validation set(test setì´ ì ˆëŒ€ ì•„ë‹ˆë‹¤.)ê³¼ k-1ê°œì˜ training setìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµì— í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤.     

ì˜ˆë¥¼ ë“¤ì–´ 100ê°œì˜ ë°ì´í„° ì…‹ì„ 10ê°œì˜ foldë¡œ ë‚˜ëˆ„ê³  ëª¨ë¸Aì— ëŒ€í•´ì„œëŠ” 1~9 fold ë°ì´í„°ì…‹ì„ training setìœ¼ë¡œ í™œìš©í•˜ê³  10 fold ë°ì´í„°ì…‹ì„ validation setìœ¼ë¡œ í™œìš©, ëª¨ë¸ B, Cì— ëŒ€í•´ì„œ ê°ê° ë‹¤ë¥¸ validation setì„ í™œìš©í•´ ê°ê°ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ì ì ˆí•œ ëª¨ë¸ì„ íƒí•˜ëŠ” ë°©ì‹ì´ë‹¤.    
ì£¼ë¡œ [hyperparameter](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)(learing rate...)ì™€ ê°™ì€ ëª¨ë¸ì˜ íŠ¹ì§•ì„ í‰ê°€í•˜ê³  ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.    

<br></br>

## Bias-Variance tradeoff
<hr/>
ì£¼ì–´ì§„ ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ìˆëŠ” ê²½ìš° Biasì™€ VarianceëŠ” ë‹¤ìŒì˜ ê´€ê³„ë¡œ ì¸í•´ ë‘˜ë‹¤ ë™ì‹œì— ì¤„ì´ëŠ” ëª¨ë¸ì„ ì°¾ëŠ” ê²ƒì€ í˜ë“¤ë‹¤.    

Cost = Bias^2 + Variance + Noise   
> $\begin{aligned} \mathbb{E}\left[(t-\hat{f})^{2}\right] &=\mathbb{E}\left[(t-f+f-\hat{f})^{2}\right] \\ &=\cdots \\ &=\mathbb{E}\left[\left(f-\mathbb{E}[\hat{f}]^{2}\right)^{2}\right]+\mathbb{E}\left[(\mathbb{E}[\hat{f}]-\hat{f})^{2}\right]+\mathbb{E}[\epsilon] \end{aligned}$

<br></br>



## BootStrapping
<hr/>
í•™ìŠµë°ì´í„°ê°€ ìˆì„ ë•Œ sub samplingì„ í†µí•´ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.    


<br><br/>



## Bagging & Boosting
<hr/>
Bagging (Bootstrapping aggregating) : ë°ì´í„°ë¥¼ bootstraping í•˜ì—¬ ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ë¡œ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤.   
Dectreteí•œ ì˜ˆì¸¡ê°’ì€ vote, ì—°ì† ë°ì´í„°ëŠ” í‰ê· ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë„ì¶œí•œë‹¤.    

Related : [Random Forest](https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8)

Boosting : Baggingê³¼ ë™ì¼í•˜ë‚˜ ê° ëª¨ë¸ì´ ë…ë¦½ì ì¸ í˜•íƒœê°€ ì´ë‹Œ Sequentialí•˜ê²Œ ë°°ì¹˜í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ weak learner ë¥¼ í†µí•´ 1ê°œì˜ strong learnerë¥¼ ë§Œë“œëŠ” ë°©ì‹ì´ë‹¤.      




<br><br/>