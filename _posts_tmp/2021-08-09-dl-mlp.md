---
title: "DeepLearning - Neural Network, MLP"
categories:
  - Calculus
tags:
  - calculas
  - Gradient_descent
  - 
last_modified_at: 2021-08-03T00:00:00-00:00
---

affine transformation
활성 함수
   ReLU (Rectified Linear Unit)
   Sigmoid, Hyperbolic Hy
   -> ? 선택하는 기준
   -> Non-Linear를 실현하기 위한 수단( 왜 Non Linear어여햐 하는가)
Loss Function
   - 어떤 성질을 가지며 왜 사용되어야 하는지 잘 고민해야한다.
   - 예를 들어 MSE는 특이값이 있는경우 망가질수 있음
   - CE, MSE, MLE

nn.Module.eval()의미



https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/
활성함수를 선택하는 기준
  두가지 영역
    히든 레이어에서의 활성함수
      뉴럴네트워크가 잘 학습할수 있도록

    아웃풋 레이어에서의 활성함수
      모델로 예측하고자 하는 목적에 따라

## Summary 🤙
---

## Index 👀       
  * [Why](#why)
  * [Definition](#definition)
  * [Feature](#feature)
  * [Use Cases](#use-cases)
  * [Related](#related)
  * [Implementation](#implementation)

## Why 🤷
---

## Definition 🧑‍🏫
---

## Feature 👍
---

## Use Cases 🪢
---

## Related 🧶
---

## Implementation 😎
---
```python
print('hello world')
```